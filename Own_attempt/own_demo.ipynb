{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cube_env import CubeEnv\n",
    "from gflownet import GFlowNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "env = CubeEnv()\n",
    "agent = GFlowNet(env)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "2"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.dim"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 985us/step\n",
      "32/32 [==============================] - 0s 943us/step\n",
      "32/32 [==============================] - 0s 909us/step\n",
      "32/32 [==============================] - 0s 955us/step\n",
      "32/32 [==============================] - 0s 985us/step\n",
      "32/32 [==============================] - 0s 947us/step\n",
      "32/32 [==============================] - 0s 931us/step\n",
      "32/32 [==============================] - 0s 956us/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 841us/step\n",
      "32/32 [==============================] - 0s 928us/step\n",
      "32/32 [==============================] - 0s 922us/step\n",
      "32/32 [==============================] - 0s 910us/step\n",
      "32/32 [==============================] - 0s 876us/step\n",
      "32/32 [==============================] - 0s 838us/step\n",
      "32/32 [==============================] - 0s 896us/step\n",
      "32/32 [==============================] - 0s 863us/step\n",
      "32/32 [==============================] - 0s 861us/step\n",
      "32/32 [==============================] - 0s 862us/step\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "No gradient defined for operation'IteratorGetNext' (op type: IteratorGetNext). In general every operation must have an associated `@tf.RegisterGradient` for correct autodiff, which this op is lacking. If you want to pretend this operation is a constant in your program, you may insert `tf.stop_gradient`. This can be useful to silence the error in cases where you know gradients are not needed, e.g. the forward pass of tf.custom_gradient. Please see more details in https://www.tensorflow.org/api_docs/python/tf/custom_gradient.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mLookupError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\GitHub\\gflownet_tf2\\Own_attempt\\gflownet.py:254\u001B[0m, in \u001B[0;36mGFlowNet.train\u001B[1;34m(self, weight_path, batch_size, verbose)\u001B[0m\n\u001B[0;32m    252\u001B[0m epoch_loss_list \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m    253\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_sampler(batch_size):\n\u001B[1;32m--> 254\u001B[0m     loss_values, gradients \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgrad\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    255\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mapply_gradients(\n\u001B[0;32m    256\u001B[0m         \u001B[38;5;28mzip\u001B[39m(gradients, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mtrainable_variables \u001B[38;5;241m+\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mz0])\n\u001B[0;32m    257\u001B[0m     )\n\u001B[0;32m    258\u001B[0m     losses_batch \u001B[38;5;241m=\u001B[39m [sample \u001B[38;5;28;01mfor\u001B[39;00m sample \u001B[38;5;129;01min\u001B[39;00m loss_values]\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\GitHub\\gflownet_tf2\\Own_attempt\\gflownet.py:280\u001B[0m, in \u001B[0;36mGFlowNet.grad\u001B[1;34m(self, batch)\u001B[0m\n\u001B[0;32m    274\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Calculate gradients based on loss function values. Notice the z0 value is\u001B[39;00m\n\u001B[0;32m    275\u001B[0m \u001B[38;5;124;03malso considered during training.\u001B[39;00m\n\u001B[0;32m    276\u001B[0m \u001B[38;5;124;03m:param batch: (tuple of ndarrays) Output from self.train_gen() (positions, rewards)\u001B[39;00m\n\u001B[0;32m    277\u001B[0m \u001B[38;5;124;03m:return: (tuple) (loss, gradients)\u001B[39;00m\n\u001B[0;32m    278\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    279\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mGradientTape() \u001B[38;5;28;01mas\u001B[39;00m tape:\n\u001B[1;32m--> 280\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrajectory_balance_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    281\u001B[0m     grads \u001B[38;5;241m=\u001B[39m tape\u001B[38;5;241m.\u001B[39mgradient(loss, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mtrainable_variables \u001B[38;5;241m+\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mz0])\n\u001B[0;32m    282\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss, grads\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\GitHub\\gflownet_tf2\\Own_attempt\\gflownet.py:218\u001B[0m, in \u001B[0;36mGFlowNet.trajectory_balance_loss\u001B[1;34m(self, batch)\u001B[0m\n\u001B[0;32m    216\u001B[0m losses \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m    217\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m reward, position \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(rewards, positions):\n\u001B[1;32m--> 218\u001B[0m     trajectory, backward_actions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msample_trajectories_bakwards\u001B[49m\u001B[43m(\u001B[49m\u001B[43mposition\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    219\u001B[0m     tf_trajectory \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mconvert_to_tensor(trajectory, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfloat32\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    220\u001B[0m     \u001B[38;5;66;03m# For each position in the trajectory, get the forward and backward\u001B[39;00m\n\u001B[0;32m    221\u001B[0m     \u001B[38;5;66;03m# policies:\u001B[39;00m\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\GitHub\\gflownet_tf2\\Own_attempt\\gflownet.py:147\u001B[0m, in \u001B[0;36mGFlowNet.sample_trajectories_bakwards\u001B[1;34m(self, position)\u001B[0m\n\u001B[0;32m    145\u001B[0m current_position \u001B[38;5;241m=\u001B[39m position\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[0;32m    146\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m keep_going:\n\u001B[1;32m--> 147\u001B[0m     model_backward_logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mposition_one_hot\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m    148\u001B[0m     model_backward_probs \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mmath\u001B[38;5;241m.\u001B[39mexp(model_backward_logits)\n\u001B[0;32m    150\u001B[0m     normalised_actions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmask_and_norm_actions(current_position, model_backward_probs)\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\GitHub\\gflownet_tf2\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\GitHub\\gflownet_tf2\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_util.py:639\u001B[0m, in \u001B[0;36m_GradientsHelper\u001B[1;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001B[0m\n\u001B[0;32m    637\u001B[0m       grad_fn \u001B[38;5;241m=\u001B[39m func_call\u001B[38;5;241m.\u001B[39mpython_grad_func\n\u001B[0;32m    638\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 639\u001B[0m       \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m(\n\u001B[0;32m    640\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo gradient defined for operation\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    641\u001B[0m           \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mop\u001B[38;5;241m.\u001B[39mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m (op type: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mop\u001B[38;5;241m.\u001B[39mtype\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m). \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    642\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIn general every operation must have an associated \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    643\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`@tf.RegisterGradient` for correct autodiff, which this \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    644\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mop is lacking. If you want to pretend this \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    645\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moperation is a constant in your program, you may insert \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    646\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`tf.stop_gradient`. This can be useful to silence the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    647\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124merror in cases where you know gradients are not needed, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    648\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124me.g. the forward pass of tf.custom_gradient. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    649\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease see more details in \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    650\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://www.tensorflow.org/api_docs/python/tf/custom_gradient.\u001B[39m\u001B[38;5;124m\"\u001B[39m)  \u001B[38;5;66;03m# pylint: disable=line-too-long\u001B[39;00m\n\u001B[0;32m    651\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m loop_state:\n\u001B[0;32m    652\u001B[0m   loop_state\u001B[38;5;241m.\u001B[39mEnterGradWhileContext(op, before\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[1;31mLookupError\u001B[0m: No gradient defined for operation'IteratorGetNext' (op type: IteratorGetNext). In general every operation must have an associated `@tf.RegisterGradient` for correct autodiff, which this op is lacking. If you want to pretend this operation is a constant in your program, you may insert `tf.stop_gradient`. This can be useful to silence the error in cases where you know gradients are not needed, e.g. the forward pass of tf.custom_gradient. Please see more details in https://www.tensorflow.org/api_docs/python/tf/custom_gradient."
     ]
    }
   ],
   "source": [
    "agent.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent.model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
