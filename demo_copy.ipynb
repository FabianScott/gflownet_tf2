{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f2bc6c3-fbfa-49b1-9456-c8650367ac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gfn import GFNAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03e0c075-642f-4ecf-b7c9-1163acf421f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = GFNAgent(epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c24541-492d-4fa8-87ec-4574ef949a1a",
   "metadata": {},
   "source": [
    "First, let's take a look at the environment. The default is a 2D 8x8 grid with high reward in the corners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e79685-b5fb-4f66-8740-6a7554137008",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.env.plot_reward_2d()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739de680-77d5-42df-9710-6738b558d11b",
   "metadata": {},
   "source": [
    "We can also look at the model structure. Notice that in this implementation, the learned parameter `z0` is separate from the neural net, and that the neural net has two output \"heads\": `foward_policy` and `backward_policy`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c11209-0661-4c0e-bb39-042d0193450c",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b55378-7021-41f5-9a4a-7514153ace10",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.z0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba135c4-339d-4f18-981b-c3888be3eb80",
   "metadata": {},
   "source": [
    "For this demonstration, we'll just show that the GFlowNet can learn a policy that generates trajectories proportional to the reward. To do that, we'll first sample a large training set using the untrained, random policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adc788c-b3fa-4329-8b65-30a50cd24da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.sample(5000)\n",
    "agent.plot_sampled_data_2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01d22b5-d758-4055-a91e-598f4e6d9fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_modes, u_positions = agent.count_modes()\n",
    "print(f'There are {u_modes} unique modes and {u_positions} unique positions in the training data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398492c9-9295-46ae-82f7-2d4b0665fabe",
   "metadata": {},
   "source": [
    "Before training, the policy is uniform. The likelihood of transitioning vertically or laterally (arrows) or terminating (red octogon) is essentially uniform at every point.\n",
    "\n",
    "The probability of terminating at each position is plotted below, and we can see that without training, it looks nothing like the reward environment we plotted above. The termination probabilities are large enough, that any trajectory is unlikely to leave the origin (bottom left)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840ecddf-34ed-4d44-9c67-216e4ab5ae0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.plot_policy_2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8a739d-1bf8-4697-b968-6583f0ed71bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_error_before = agent.compare_env_to_model_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10528439-d61a-4efa-b7b3-88552dbc3573",
   "metadata": {},
   "source": [
    "Let's train it and see if we can do better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea62ecb9-1129-4b64-b29c-aa8f3dcdcbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6957008c-c79c-4154-b36b-07f2e33f28b8",
   "metadata": {},
   "source": [
    "Let's plot the trained policy and sample from it to get a probability distribution over the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e33d9b-19a7-4aa6-bc9d-37f48dfed5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.plot_policy_2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84d9f3c-af87-40bb-8158-5c1e348d3968",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_error_after = agent.compare_env_to_model_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d81e2a-48ab-4ac7-a3db-4c64510b95c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'L1 error before {l1_error_before:.2f} and after {l1_error_after:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6e2c0f-ef2c-44d3-bad7-c4c5a188f419",
   "metadata": {},
   "source": [
    "While not perfect, the model has certainly learned to generate trajectories through the environment with probability proportional to the reward! It's a far better approximation than the untrained policy, and you can imagine that it would get better with some tweaks (e.g. more training, different optimizer, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 150ms/step\n",
      "[[0.376489   0.25777655 0.36573445]\n",
      " [0.37648899 0.25777654 0.36573447]]\n",
      "[[0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "[[0.34950482 0.31951355 0.33098163]\n",
      " [0.37648899 0.25777654 0.36573447]]\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[[0.34950482 0.31951355 0.33098163]\n",
      " [0.33333333 0.33333333 0.33333333]]\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]]\n",
      "[[0. 0. 1.]\n",
      " [1. 0. 0.]]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]]\n",
      "[[0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]]\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]]\n",
      "[[0. 0. 1.]\n",
      " [1. 0. 0.]]\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]]\n",
      "[[0. 1. 0.]\n",
      " [0. 1. 0.]]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]]\n",
      "[[0. 0. 1.]\n",
      " [0. 1. 0.]]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]]\n",
      "[[0. 1. 0.]\n",
      " [1. 0. 0.]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]]\n",
      "[[0. 1. 0.]\n",
      " [1. 0. 0.]]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]]\n",
      "[[0. 1. 0.]\n",
      " [0. 1. 0.]]\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]]\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]]\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]]\n",
      "[[0. 1. 0.]\n",
      " [0. 1. 0.]]\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "[[0.36289849 0.33576324 0.30133828]\n",
      " [0.36289849 0.33576324 0.30133828]]\n",
      "[[0. 0. 1.]\n",
      " [0. 1. 0.]]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[[0.36289849 0.33576324 0.30133828]\n",
      " [0.35456485 0.36293954 0.28249562]]\n",
      "[[0. 0. 1.]\n",
      " [1. 0. 0.]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.34185658 0.33766634 0.32047708]]\n",
      "[[0. 0. 1.]\n",
      " [0. 1. 0.]]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.31775659 0.28024775 0.40199566]]\n",
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.31775659 0.28024775 0.40199566]]\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]]\n",
      "[[0. 1. 0.]\n",
      " [1. 0. 0.]]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]]\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]]\n",
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]]\n",
      "[[0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]]\n",
      "[[0. 0. 1.]\n",
      " [0. 1. 0.]]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]]\n",
      "[[0. 0. 1.]\n",
      " [1. 0. 0.]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]]\n",
      "[[0. 0. 1.]\n",
      " [0. 1. 0.]]\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]]\n",
      "[[0. 0. 1.]\n",
      " [0. 1. 0.]]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]]\n",
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]]\n",
      "[[0. 1. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from gfn import GFNAgent\n",
    "agent = GFNAgent(epochs=200)\n",
    "l = agent.sample_trajectories(2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "l[0].shape, l[1].shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "one_hot_positions = tf.one_hot(l[0][0], 8, axis=-1)\n",
    "one_hot_positions[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "for i, action in enumerate(tfd.Categorical(probs=l[1][0]).sample().numpy()):\n",
    "    print(action == (agent.action_space - 1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "l[1][0]"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
